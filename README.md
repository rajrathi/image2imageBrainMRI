# Unpaired Image to Image Translation for MRI using CycleGANs
 This repository contains the project about Image Translation using CycleGANs for converting type T1 MRI into type T2 MRI
 
## Problem Statement
 Misdiagnosis in the medical field is a very serious issue but it’s also uncomfortably common to occur. Imaging procedures in the medical field requires an expert radiologist’s opinion  since interpreting them is not a simple binary process ( Normal or Abnormal). Even so, one radiologist may see something that another does not. This can lead to conflicting reports and make it difficult to effectively recommend treatment options to the patient.

One of the complicated tasks in medical imaging is to diagnose MRI(Magnetic Resonance Imaging). Sometimes to interpret the scan, the radiologist needs different variations of the imaging which can drastically enhance the accuracy of diagnosis by providing practitioners with a more comprehensive understanding.

 
But to have access to different imaging is difficult and expensive. With the help of deep learning, we can use style transfer to generate artificial MRI images of different contrast levels from existing MRI scans. This will help to provide a better diagnosis with the help of an additional image.

As scanning for multiple type of MRIs for single diagnosis requires time, effort and money as well. So, the main objective of the project was to convert the type T1 MRI into type T2 MRI, so that two types of MRI provide better insights for diagnosis using deep learning and hence avoiding time and efforts. Following is the table for highlight style for different type of elements present in human brain.

| Type	| T1 Highlight Style	| T2 Highlight Style |
| ---- | ------------------ | ------------------ |
| Water |	Dark	Very | Bright |
| Fat	| Very Bright	| Dark |
| Bone |	Dark |	Dark |
| Muscle |	Intermediate |	Dark |
| Tumours |	Intermediate |	Bright |

## Solution
 I have come up with the solution for above Problem with the help of CycleGANs (U-net Generators and Discriminators). The U-net Generators consists of downsampling which consists of Residual blocks similar to the architecture of ResNet, and upsampling which uses Conv2dTranspose layers to build image back from extracted features. 
 The Discriminitor is a simple ConvNet architecture to check the originality of the fake image generated by generator. 
 
Following are the type of losses used for calculating gradients which are then used to update weights through back propogation:

<dl>
 <dt> Adverserial Loss </dt>
  <dd> Instead of using Binary Cross Entropy as loss function the model uses Mean Squared Error for calculating loss as it signifies how much output is correct or incorrect (using each pixel value). This helps to avoid the problem for vanishing gradient and give strong gradient updates. </dd>
 <dt> Discriminator Loss </dt>
  <dd> Similar to adverserial loss, here also Mean Square Error loss funtion is used for the above mentioned purpose. </dd>
 <dt> Cycle Loss </dt>
  <dd> Cycle loss is calculated afte the complete cycle of gererating images using 2 generators (1. real -> fake 2. fake -> real). This loss is calulated between the input image and cycled image using L1 loss function.  </dd>
 <dt> Identity Loss </dt>
  <dd> Identity loss is needed for to check whether the more translation is needed or not, such that if the image is already into desired format, then identity loss should become zero. To calculate identity loss we have used L1 funtion. </dd>
</dl>
 
The model also used linear decay learning rate strategy for reducing the learning rate after a certain epoch to find the optimal minima for the neural network. Model also uses Adam Optimizer for updating weigts and biases. The project is implemented mainly using TensorFlow and various other packages.

## Results
 Following is the gif of training of the model for 200 epochs.

<img src="https://github.com/rajrathi/image2imageBrainMRI/blob/main/cyclegan.gif" alt="training gif" height="400"/>

 
 
 
